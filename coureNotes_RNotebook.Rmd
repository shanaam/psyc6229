---
title: "Statistical Modeling - Notes"
output: html_notebook
---

# 08/01/2019

## Useful things to know

The tests (on Tuesdays) will be on the test banks posted up to the Friday before.
Since this course is both about programming AND probability --> There will be both written and 


### Term project
__To do: Send Richard a proposal for your final project__

* What do I study
* What's my data
* What kind if analysis I want to do
* How will i do this?

To think about:
What is some type of data analysis I would like to do in R?  

* E.g. GLM applied to my research --> regression with stuff added to it
OR write a tutorial about something, e.g. the GLM, where you teach ANOTHER 'grad student' how to do this

* E.g. GLM does this, it can be useful for this type of data, here is how you do it

## Probability

We are __really__ bad at making decisions based on probability -- so much so that this is a field of study in itself.

* the Monty Hall problem (doors, car, goat, etc.) is a good example of this; Richard used the examples of many doors and Monty revealing all but one door to show that you should always switch.

* 2/3 chance that it is in one of the boxes that I __didn't__ pick. And now I have extra info (100% that it's not there). There's still a 2/3 chance that it's in the other one


Basic R code from first lecture --> assigning variables, loops  
CRAN is a repository of all things related to R  

* If you have only one step in your for loop, while loop, if statement, etc (e.g. print (i)), you don't need 'set braces' ({}).  

```{r eval=FALSE}
for (i in 1:5)
  print (i)
```
Above:  '1:5' can be any method of generating vectors  

```{r eval=FALSE}
for (i in rnorm(5))
  print (i)
```
rnorm(x) gets you a vector with x number of samples from the standard normal distribution

if (x) break --> breaks out of a loop when x is TRUE

```{r}
# encode the sign of a number
r <- rnorm( 1 )
if ( r < 0 ) {
	sgn <- -1
} else {
	sgn <- 1
}
cat( "the number", r, "has sign", sgn, "\n" )
```
For the above piece of code, the else __has__ to appear on the same line as }. When R runs into a } in an if statement, it checks __that line__ for else or else if, if there is nothing there, it completes the if statement.  

There are switch statements if you would have __a lot__ of if/else if statements!
If you don't have a return argument in a function, what gets returned is the __last__ thing you calculated in the funcion.


## Theory

### Probability Space

Prob. space = (Ω, E, P)
* What we know to be true about probabilities in some situation
* We have a __sample space__ (omega) = a list of ALL the things that can possibly happen. e.g. {1,2,3,4,5,6} for rolling a die. Each one is an 'outcome'. In some other sample space (in another model of rolling a die), we can have {1,2,3,4,5,6,side,corner,na}.  
+ It's worth thinking about what the sample space for your process is.  
* We also have __events__ --> all possible subsets of outcomes (e.g. {1}, {2} ...{1,2} (1 __or__ 2), {1,3}... and so on.)  
+ One event has to be all possible outcomes
+ This does'nt hold when the probability space has an infinite number of possible outcomes  
* Finally, we have a __probability measure__.  
+ a function where we give it an __event__, and it gives you a number (the probability of that event)
+ there is a P for all possibles Es

__The 3 axioms of the probability measure__.
2. P(A) >= 0
1. P(Ω) = 1      __Ω = omega (alt+ 234)__  
3. If A,B are disjoint (they dont have any outcomes in common; mutually exclusive) --> P(A U B) (U is union (either of them happening)) = P(A) + P(B)
+ e.g. P({1} U {5}) = P({1}) + P({5}) = 2/6  
+ note: of the events do overlap, you use the addition rule (you subtract out the P of the overlapping parts)  


Basic facts of probability that follow from these axioms: (you can try dereiving these using just the axioms). Drawing Venn diagrams are helpful for figuring these out.  

* P (not A) = 1 - P(A)
* 0 <= P(A) <= 1
* A subset B --> P(A) <= P(B)        __The arrow --> means 'implies' ('then'), an 'if' is implied__  
+ __This is the addition rule__  
* P(A U B) = P(A) + P (B) - P(A ∩ B)     __∩ = intersection (alt + 239)__ 


Other facts

1. Conditional probability
P(A|B) = P(A and B) / P(B)    __Venn diagram's overlapping part is the P(A and B) part here!__  
$$\frac{P(A and B)}{P(B)}$$

* You're shrinking Ω to B by saying "given"
* Try this with a dice roll: What is P(even | <=3)

Flipping the conditional probability gives you: P(A and B) = P(B)*P(A|B)
* __This is the multiplication rule__  

2. If 2 events are independent: P(A|B) = P(A).

That is, knowing if B happened gives you no information about P(A)
This simplifies the multiplication rule to P(A and B) = P(A) * P(B)  

Note: If 2 sets are disjoint, they're definitely NOT independent. Knowing if A occured gives you A LOT of information about B. That is, if A occurs, there is no way that B also occured.

Q: is even and <=3 independent? We know that it's NO. P(even) = 0.5, but if we know that it's <= 3, the P(even) changes to 1/3!  
BUT even and <= 4 ARE independent (if you plug into the formula in 2, both sides are 0.5.


***

# 22/01/2019

## Set notation  
$e_1 = \{O_1, O_2, O_3\}$  
* the order does not matter here

∩ --> closely related to AND
U --> closely related to OR

We would use AND and OR when talking about the probabilities most of the time. Because intersection and union are not really made for use in every-day language. They're kind of specifically mathematical.  

## Bayes Rule

***

# 29/01/2019

## Lists  

When you have a list of a bunch of elements + values, it is easier to visualise using `str(x)` instead of `print(x)` or `x`.  


## Dataframes  

They're basically a bunch of atomic vectors all with the same length.  
The lengths are __enforced__, so if you append one of the vectors, it will populate the others to match the lengths (with NULL?)

## [1] vs [[1]]

[1] gives you a sublist or subdataframe (gives you the first element of list/df in the same type as the parent)  
[[1]] gives you the underlying atomic vector!

## Random variables

Doesn't mean completely random. There is a distribution.  

A __real valued random variable__ is a function that takes Ω and outputs real numbers (ℝ)  
* However, this can be a little unweildly when we get more complicated experiments
* So we need a better ways of summarizing these random variables

### CDF: One way of summarizing (Cumulative distribution function)

It's a function; $F_X(x) = P (X <= x)$, where X is the random variable, x is a number
$F_X$ approaches 0 for very negative values of x (as x --> -inf)  
$F_x$ approaches 1 for very positive values of x (as x --> inf)  
Often, when you are doing more interesting statistics, they are more intuitive in terms of the CDF rather than the probability distribution function (PDF)  

for CDFs:  
* P(a <= X <= b) = F(b) - F(a)
* P(X > x) = 1 - F(x)

***

# 05/02/2019

## Indepenedence

P(A|B) = P(A)
With random variables, there are multiple interpretations of this!

1. P(X=3|Y=1) = P(X=3) (so Y being 1 gives us no information about X being 3. But this says nothing about the rest of the probability space)

2. Any x, any y; P(X=x|Y=y) = P(X=x)

3. X1, X2, X3 ...
  + If they are pairwise independent: any two
  + If they are MUTUALLY independent: even if you know any combination of Xs, it doesnt give you information about any other X. P(X1|X2, X3 ...) = P(X1)  
  + Think over Richard's example in class (X and Y are coin flips (1 or 0), Z = X XOR Y // Z = X != Y). Knowing X gives you no info on Z. BUT knowing both X and Y gives you complete information about Z! (What type of probabiliity is this?)





## Normal distributions

They are "stable":
When you add normal distributions, the mean of the new distribution = mu1 + mu2
The variance (not std) of the new distribution = var1 + var2

***

# 26/02/2019

Radiology task:

% "yes" | A          | B 
--------| -----------| ---------- 
pre     | Y: 0.70    | Y: 0.65        
-       | N: 0.70    | N: 0.50 
post    | Y: 0.90    | Y: 0.80
-       | N: 0.90    | N: 0.50 (or 0.60)

Y = hits
N = false alarms


A has 0 sensitivity, but their bias increased!
What about B though? has their sensitivity increased? How do you tell?

## Signal detection theory

Response matrix | "Y" | "N" 
----------------| ----| ----
Y               | Hit (H) | Miss (M) 
N               | False alarm (FA) | Correct rejection (CR) 

These names are a little arbritary (different in diff fields, e.g.: true positive, false positive, true negative, false negative)  
Aside: sensitivity and selectivity in some fields is NOT the same sensitivity we are talking about (they're usually talkinb about Hit rate)  

H = proportion of Y trials where subject says "Y"  
Therefore, __M = 1 - Y__, so we need to only report ONE column  
It's convention to report H and FA (easy to test; give them 100 Y and 100 N trials and make them "detect" whether it is truly Yes or No)  
__This does NOT have to be a detection task!__ It can be whether something is leaning left/right, whether a face is a M/F, etc. (even works when people also rate their confidence in their response)  

In all situations, you want sensitivity to be as high as possible. Bias on the other hand (whether you have more False alarms vs more Misses) is complicated and you have to decide which direction you rather it fall  

### Assumptions

* Your decision is based on ONE one-dimentional information: <---------------------------->  
+ You can think of this as "confidence" or "spike rate of a neuron" (low results in one response, and high results in another)  
+ This is called a "decision variable"
+ This is not always the case (your decision can be based on 2 variables, but for now, we will say it's based on one)  

* Each trial gives you a decision variable (some information). You have to determine whether this comes from a Y trial or a N trial  
+ Therefore, we can think of it as being distributed normally on the 1-D number line with a mean (which is higher for a Y trial than a N trial) and a sd  
+ On a single trial, you don't know which distribution this decision variable came from! You have to guess whether it comes from a Y distribution or a N distribution, and make your decision based on that  

* We have a fixed criterion that determines the response   
+ Where the two disctributions cross  
+ Anything above this criterion results in a "Y", anything below results in a "N"

### Sensitivity

See figure in notes  

- The area under these curves determine hit rate, false alarm rate, etc.    
- With these assumptions in mind, we can formulate sensitivity (d') and bias (c)  

Given area under the curve (say, Hit rate), you can use qnorm to calculate the x!  
- In signal detection theory, qnorm is known as z(p)  
- __z(p) answers the question:__ how many sd above the mean is x to have area P under the curve to the left of it.  
- i.e., think of this in terms of z-score (z = (x-mu/sd))
- __useful:__ z(1 - P) = -z(P)


### ROC curve

Acronym: Receiver operating characteristic curve
- A plot that plots Hits on the y-axis and False alarms on the x-axis
- Signal detection theory predicts an ROC curve like ones we see in real life after conducting some experiments  
- Other models of decision making on the other hand, like the strawman one, predicts a different curve, that doesn't come about  

***

# 05/03/2019

In 2AFCs:
$$d' = \frac{z(H) - z(FA)}{\sqrt{2}}$$

## Cue combination











***

# Useful Notes

matrix[-1, ] : everything __except__ the first row

setwd( '..' )                  # goes up one directory  
rm( x )                        # remove variable x  
rm( list=ls() )                # remove all variables  
save( x, y, file='datafile.Rdata' )  # save variables  
load( 'datafile.Rdata' )             # load variables  
unlink( 'datafile.Rdata' )           # delete a file  

rnorm( length, mean, sd)        # gives you length amount of values from a normal distribution  
dnorm (range, mean, sd)         # gives you a density function (bell shaped curve around mean)  

you can name elements of an atomic vector! e.g. c(pse=0.5, jnd=0.5)

## How R names probability related functions

r norm --> The r indicates a random number generator, norm indicates a normal distribution  
p norm --> p indicates a CDF  
d norm --> d indicates a density function  

p unif --> CDF of the uniform distribution  

### Models in R

__col1 ~ pnorm( col2, parameter1, parameter1)__

Meaning: col1 (of a dataframe) varies on ... right side



